# Автоматическое Аннотирование Изображений с использованием архитектуры CNN-LSTM и ResNet-50

Это дипломный проект, разработанный в рамках курса в **Astana IT University**. Система предназначена для автоматического создания описаний (аннотаций) для изображений с использованием комбинации сверточной нейронной сети (CNN) для анализа изображений и рекуррентной нейронной сети (LSTM) для генерации текста.

## Авторы

*   **Максим Азарнов**
*   **Динмухамед Лайков**
*   **Милана Стёпкина**

## Ключевые Особенности

*   **Архитектура Encoder-Decoder:** Используется для преобразования визуальной информации в текстовое описание.
*   **Визуальный Энкодер (Encoder):** Применяется предобученная модель **ResNet-50** для извлечения глубоких признаков (feature vectors) из изображений.
*   **Текстовый Декодер (Decoder):** Используется сеть **LSTM** для генерации последовательного, грамматически корректного текста на основе признаков изображения.
*   **Оценка качества:** Производительность модели оценивается с помощью метрик **BLEU** (BLEU-1, BLEU-2, BLEU-3, BLEU-4).

## Архитектура Модели

Проект реализует классическую архитектуру Encoder-Decoder для задач Image Captioning.

1.  **Энкодер (CNN):** Изображение подается на вход модели ResNet-50 (без верхнего классификационного слоя). С помощью Global Average Pooling извлекается вектор признаков размерностью 2048, который представляет собой семантическое сжатие содержимого изображения.
2.  **Декодер (LSTM):** Полученный вектор признаков проходит через полносвязный слой и подается как начальное состояние в LSTM-декодер. Декодер последовательно, слово за словом, генерирует описание, используя также информацию о предыдущем сгенерированном слове.

```
Изображение -> ResNet-50 -> Вектор признаков (2048) -> Dense Layer -> LSTM Декодер -> Аннотация (текст)
```

## Результаты

Модель была обучена в течение 35 эпох. Итоговые результаты значительно превосходят случайную базовую модель (Random Baseline).

### Количественная оценка (BLEU Scores)

| Метрика | Оценка Модели | Случайная Модель (Baseline) |
| :--- | :--- | :--- |
| **BLEU-1** | 0.5805 | 0.343 |
| **BLEU-2** | 0.3783 | 0.122 |
| **BLEU-3** | 0.2502 | 0.044 |
| **BLEU-4** | 0.1600 | 0.015 |

### Примеры сгенерированных аннотаций

> **ВАЖНО:** Замените `path/to/your/image.jpg` на реальные пути к вашим изображениям в репозитории.

**Пример 1**
![Две собаки бегут по траве](examples/dog_example.jpg)
*   **Сгенерировано:** a dog is running through the grass.
*   **Оригинал:** a brown and a brown dog are running. | two dogs running in the dirt.

**Пример 2**
![Мужчина на водных лыжах](examples/waterski_example.jpg)
*   **Сгенерировано:** a man in a wetsuit is surfing on a surfboard.
*   **Оригинал:** a guy on a waterskiing board is doing a stunt. | a man on a wakeboard is jumping over the wake and trying to grab the board with one hand.

**Пример 3**
![Велосипедист на трамплине](examples/bike_example.jpg)
*   **Сгенерировано:** a man in a red shirt is riding a bike on a dirt bike.
*   **Оригинал:** a bicycle racer runs up a ramp with his bike. | a biker is running with his bike in his hands up a ramp.

## Набор данных (Dataset)

Для обучения и оценки использовался датасет **Flickr8k**. Он содержит 8,091 изображение, каждое из которых имеет 5 различных аннотаций, написанных человеком. Это позволяет обучать модель на разнообразных текстовых описаниях одного и того же визуального контента.

## Как запустить проект (Getting Started)

### 1. Предварительные требования

*   Python 3.8+
*   TensorFlow / Keras
*   Pandas, NumPy
*   OpenCV-Python

### 2. Установка

1.  **Клонируйте репозиторий:**
    ```bash
    git clone https://github.com/YourUsername/YourRepositoryName.git
    cd YourRepositoryName
    ```

2.  **Создайте и активируйте виртуальное окружение:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # Для Linux/macOS
    # venv\Scripts\activate    # Для Windows
    ```

3.  **Установите зависимости:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Загрузите датасет:** Скачайте датасет Flickr8k и распакуйте его в папку `data/`.

### 3. Использование

*   **Для обучения модели:**
    ```bash
    python train.py
    ```

*   **Для генерации аннотации к вашему изображению:**
    ```bash
    python predict.py --image path/to/your/image.jpg
    ```

## Дальнейшие улучшения (Future Work)

*   **Механизмы Внимания (Attention):** Внедрение механизма внимания позволит модели фокусироваться на наиболее релевантных частях изображения при генерации каждого слова.
*   **Beam Search:** Использование Beam Search вместо жадного поиска (Greedy Search) при декодировании для генерации более качественных и разнообразных аннотаций.
*   **Расширение словаря:** Использование большего набора данных для увеличения размера словаря и повышения описательной способности модели.
*   **Трансформеры:** Полный переход на архитектуру Трансформеров (например, модель `ViT` + `GPT-2`) как более современную альтернативу CNN-LSTM.

## Лицензия

Проект распространяется под лицензией MIT. Смотрите файл `LICENSE` для получения дополнительной информации.
